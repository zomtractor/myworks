{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T12:31:50.064825Z",
     "start_time": "2025-09-09T12:11:32.240200Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"专家网络 - 替代标准FFN\"\"\"\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MoEGatingNetwork(nn.Module):\n",
    "    \"\"\"MoE门控网络\"\"\"\n",
    "    def __init__(self, dim, num_experts, k=2):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.gate = nn.Linear(dim, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, dim)\n",
    "        logits = self.gate(x)  # (batch, seq_len, num_experts)\n",
    "        weights = self.softmax(logits)\n",
    "        \n",
    "        # 选择top-k个专家\n",
    "        topk_weights, topk_indices = torch.topk(weights, self.k, dim=-1)\n",
    "        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # 创建稀疏掩码\n",
    "        mask = torch.zeros_like(weights).scatter(-1, topk_indices, 1)\n",
    "        \n",
    "        return weights, mask, topk_weights, topk_indices\n",
    "\n",
    "class MoEFFN(nn.Module):\n",
    "    \"\"\"MoE前馈层 - 替代标准FFN\"\"\"\n",
    "    def __init__(self, dim, hidden_dim, num_experts=4, k=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        \n",
    "        # 创建专家网络\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(dim, hidden_dim, dropout) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # 创建门控网络\n",
    "        self.gating = MoEGatingNetwork(dim, num_experts, k)\n",
    "        \n",
    "        # 辅助损失系数\n",
    "        self.aux_loss_coef = 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        \n",
    "        # 通过门控网络获取权重和掩码\n",
    "        weights, mask, topk_weights, topk_indices = self.gating(x)\n",
    "        \n",
    "        # 初始化输出\n",
    "        expert_outputs = torch.zeros(\n",
    "            batch_size, seq_len, self.num_experts, self.dim, \n",
    "            device=x.device, dtype=x.dtype\n",
    "        )\n",
    "        \n",
    "        # 计算每个专家的输出\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # 找到需要当前专家的token索引\n",
    "            idx = torch.where(mask[..., i] == 1)\n",
    "            if len(idx[0]) > 0:\n",
    "                # 处理这些token\n",
    "                expert_outputs[idx[0], idx[1], i] = expert(x[idx[0], idx[1]])\n",
    "        \n",
    "        # 聚合专家输出\n",
    "        output = torch.zeros_like(x)\n",
    "        for i in range(self.k):\n",
    "            expert_idx = topk_indices[..., i]  # (batch, seq_len)\n",
    "            for b in range(batch_size):\n",
    "                for s in range(seq_len):\n",
    "                    expert_id = expert_idx[b, s]\n",
    "                    output[b, s] += topk_weights[b, s, i] * expert_outputs[b, s, expert_id]\n",
    "        \n",
    "        # 计算辅助损失（负载均衡损失）\n",
    "        aux_loss = self._load_balancing_loss(weights, mask)\n",
    "        \n",
    "        return output, aux_loss\n",
    "    \n",
    "    def _load_balancing_loss(self, weights, mask):\n",
    "        \"\"\"计算负载均衡损失\"\"\"\n",
    "        # 计算每个专家的使用率\n",
    "        expert_usage = mask.float().mean(0).mean(0)  # (num_experts)\n",
    "        \n",
    "        # 计算每个token的专家权重\n",
    "        weight_sum = weights.mean(0).mean(0)  # (num_experts)\n",
    "        \n",
    "        # 负载均衡损失\n",
    "        aux_loss = torch.sum(expert_usage * weight_sum) * self.num_experts\n",
    "        return aux_loss * self.aux_loss_coef\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"自注意力机制\"\"\"\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        \n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "        \n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        \n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer块 - 使用MoE替代标准FFN\"\"\"\n",
    "    def __init__(self, dim, heads, dim_head, mlp_dim, num_experts=4, k=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, dim_head, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.moe_ffn = MoEFFN(dim, mlp_dim, num_experts, k, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 自注意力\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        \n",
    "        # MoE前馈网络\n",
    "        ffn_out, aux_loss = self.moe_ffn(self.norm2(x))\n",
    "        x = x + ffn_out\n",
    "        \n",
    "        return x, aux_loss\n",
    "\n",
    "class ViTMoEForRestoration(nn.Module):\n",
    "    \"\"\"Vision Transformer with MoE for Image Restoration\"\"\"\n",
    "    def __init__(self, image_size, patch_size, dim, depth, heads, mlp_dim, \n",
    "                 num_experts=4, k=2, channels=3, dim_head=64, dropout=0.1, emb_dropout=0.1):\n",
    "        super().__init__()\n",
    "        image_height, image_width = image_size if isinstance(image_size, tuple) else (image_size, image_size)\n",
    "        patch_height, patch_width = patch_size if isinstance(patch_size, tuple) else (patch_size, patch_size)\n",
    "        \n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n",
    "            'Image dimensions must be divisible by the patch size.'\n",
    "        \n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.image_size = image_size\n",
    "        self.channels = channels\n",
    "        \n",
    "        # Patch嵌入\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            nn.Conv2d(channels, dim, kernel_size=patch_size, stride=patch_size),\n",
    "            nn.Flatten(2)  # (B, dim, H*W) -> (B, H*W, dim)\n",
    "        )\n",
    "        \n",
    "        # 位置编码\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        \n",
    "        # Transformer层\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(TransformerBlock(\n",
    "                dim, heads, dim_head, mlp_dim, num_experts, k, dropout\n",
    "            ))\n",
    "        \n",
    "        # 输出层 - 重建图像\n",
    "        self.to_pixel = nn.Sequential(\n",
    "            nn.Linear(dim, patch_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # 上采样卷积 - 确保输出尺寸与输入一致\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(dim, channels, kernel_size=patch_size, stride=patch_size),\n",
    "            nn.Tanh()  # 限制输出范围到[-1, 1]\n",
    "        )\n",
    "        \n",
    "        # 存储辅助损失\n",
    "        self.aux_losses = []\n",
    "        \n",
    "    def forward(self, img):\n",
    "        # 提取patch嵌入\n",
    "        x = self.to_patch_embedding(img)  # (B, dim, num_patches)\n",
    "        x = x.permute(0, 2, 1)  # (B, num_patches, dim)\n",
    "        \n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        # 添加位置编码\n",
    "        x += self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 通过Transformer层\n",
    "        self.aux_losses = []  # 重置辅助损失\n",
    "        for layer in self.layers:\n",
    "            x, aux_loss = layer(x)\n",
    "            self.aux_losses.append(aux_loss)\n",
    "        \n",
    "        # 重建图像\n",
    "        # 方法1: 直接重建每个patch的像素值\n",
    "        # pixels = self.to_pixel(x)  # (B, num_patches, patch_dim)\n",
    "        # out = pixels.reshape(b, self.num_patches, self.channels, self.patch_size, self.patch_size)\n",
    "        # out = out.permute(0, 2, 1, 3, 4)  # (B, C, num_patches, patch_size, patch_size)\n",
    "        # h = w = int(self.num_patches ** 0.5)\n",
    "        # out = out.reshape(b, self.channels, h, w, self.patch_size, self.patch_size)\n",
    "        # out = out.permute(0, 1, 2, 4, 3, 5)  # (B, C, h, patch_size, w, patch_size)\n",
    "        # out = out.reshape(b, self.channels, h * self.patch_size, w * self.patch_size)\n",
    "        \n",
    "        # 方法2: 使用转置卷积上采样\n",
    "        x = x.permute(0, 2, 1)  # (B, dim, num_patches)\n",
    "        h = w = int(self.num_patches ** 0.5)\n",
    "        x = x.reshape(b, -1, h, w)  # (B, dim, h, w)\n",
    "        out = self.upsample(x)  # (B, C, H, W)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_aux_loss(self):\n",
    "        \"\"\"获取所有辅助损失的总和\"\"\"\n",
    "        return sum(self.aux_losses) if self.aux_losses else torch.tensor(0.0)\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 超参数\n",
    "    image_size = 64\n",
    "    patch_size = 8\n",
    "    channels = 3\n",
    "    dim = 256\n",
    "    depth = 6\n",
    "    heads = 8\n",
    "    mlp_dim = 512\n",
    "    num_experts = 4\n",
    "    k = 2\n",
    "    \n",
    "    # 创建ViT-MoE图像恢复模型\n",
    "    model = ViTMoEForRestoration(\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        dim=dim,\n",
    "        depth=depth,\n",
    "        heads=heads,\n",
    "        mlp_dim=mlp_dim,\n",
    "        num_experts=num_experts,\n",
    "        k=k,\n",
    "        channels=channels\n",
    "    )\n",
    "    \n",
    "    # 创建示例输入\n",
    "    x = torch.randn(2, channels, image_size, image_size)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = model(x)\n",
    "    aux_loss = model.get_aux_loss()\n",
    "    \n",
    "    print(f\"输入形状: {x.shape}\")\n",
    "    print(f\"输出形状: {output.shape}\")\n",
    "    print(f\"辅助损失: {aux_loss.item():.4f}\")\n",
    "    \n",
    "    # 模拟训练步骤 - 使用均方误差损失\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 假设的目标图像 (与输入相同尺寸的恢复目标)\n",
    "    target = torch.randn_like(x)\n",
    "    \n",
    "    # 计算主损失\n",
    "    main_loss = criterion(output, target)\n",
    "    \n",
    "    # 总损失 = 主损失 + 辅助损失\n",
    "    total_loss = main_loss + aux_loss\n",
    "    \n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"主损失: {main_loss.item():.4f}\")\n",
    "    print(f\"总损失: {total_loss.item():.4f}\")\n",
    "    \n",
    "    # 打印模型参数数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"总参数量: {total_params:,}\")"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 283\u001B[0m\n\u001B[0;32m    280\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m2\u001B[39m, channels, image_size, image_size)\n\u001B[0;32m    282\u001B[0m \u001B[38;5;66;03m# 前向传播\u001B[39;00m\n\u001B[1;32m--> 283\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    284\u001B[0m aux_loss \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_aux_loss()\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m输入形状: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[4], line 225\u001B[0m, in \u001B[0;36mViTMoEForRestoration.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maux_losses \u001B[38;5;241m=\u001B[39m []  \u001B[38;5;66;03m# 重置辅助损失\u001B[39;00m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 225\u001B[0m     x, aux_loss \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    226\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maux_losses\u001B[38;5;241m.\u001B[39mappend(aux_loss)\n\u001B[0;32m    228\u001B[0m \u001B[38;5;66;03m# 重建图像\u001B[39;00m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;66;03m# 方法1: 直接重建每个patch的像素值\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;66;03m# pixels = self.to_pixel(x)  # (B, num_patches, patch_dim)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    237\u001B[0m \n\u001B[0;32m    238\u001B[0m \u001B[38;5;66;03m# 方法2: 使用转置卷积上采样\u001B[39;00m\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[4], line 155\u001B[0m, in \u001B[0;36mTransformerBlock.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    152\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x))\n\u001B[0;32m    154\u001B[0m \u001B[38;5;66;03m# MoE前馈网络\u001B[39;00m\n\u001B[1;32m--> 155\u001B[0m ffn_out, aux_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmoe_ffn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    156\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m ffn_out\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x, aux_loss\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[4], line 75\u001B[0m, in \u001B[0;36mMoEFFN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     69\u001B[0m expert_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\n\u001B[0;32m     70\u001B[0m     batch_size, seq_len, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_experts, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdim, \n\u001B[0;32m     71\u001B[0m     device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype\n\u001B[0;32m     72\u001B[0m )\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# 计算每个专家的输出\u001B[39;00m\n\u001B[1;32m---> 75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, expert \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43menumerate\u001B[39;49m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperts):\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;66;03m# 找到需要当前专家的token索引\u001B[39;00m\n\u001B[0;32m     77\u001B[0m     idx \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mwhere(mask[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, i] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(idx[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     79\u001B[0m         \u001B[38;5;66;03m# 处理这些token\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[4], line 75\u001B[0m, in \u001B[0;36mMoEFFN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     69\u001B[0m expert_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\n\u001B[0;32m     70\u001B[0m     batch_size, seq_len, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_experts, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdim, \n\u001B[0;32m     71\u001B[0m     device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype\n\u001B[0;32m     72\u001B[0m )\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# 计算每个专家的输出\u001B[39;00m\n\u001B[1;32m---> 75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, expert \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43menumerate\u001B[39;49m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperts):\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;66;03m# 找到需要当前专家的token索引\u001B[39;00m\n\u001B[0;32m     77\u001B[0m     idx \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mwhere(mask[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, i] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(idx[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     79\u001B[0m         \u001B[38;5;66;03m# 处理这些token\u001B[39;00m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1103\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1061\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2024.1.3\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2024.1.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1196\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1193\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1196\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2024.1.3\\plugins\\python\\helpers\\pydev\\pydevd.py:1211\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1208\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1210\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1211\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1213\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1215\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "31117ebebfa1335d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
