{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# --- 简化的 StripConv 用于测试 --- #\n",
    "class StripConvTest(nn.Module):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super().__init__()\n",
    "        pad = kernel_size // 2\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv_h = nn.Conv2d(1, 1, (1, kernel_size), padding=(0, pad), bias=False)\n",
    "        self.conv_v = nn.Conv2d(1, 1, (kernel_size, 1), padding=(pad, 0), bias=False)\n",
    "        self.conv_d1 = nn.Conv2d(1, 1, kernel_size, padding=pad, bias=False)  # 主对角线\n",
    "        self.conv_d2 = nn.Conv2d(1, 1, kernel_size, padding=pad, bias=False)  # 副对角线\n",
    "\n",
    "        # 初始化卷积核为全1，方便观察\n",
    "        nn.init.constant_(self.conv_h.weight, 1.0)\n",
    "        nn.init.constant_(self.conv_v.weight, 1.0)\n",
    "        nn.init.constant_(self.conv_d1.weight, 0.0)\n",
    "        nn.init.constant_(self.conv_d2.weight, 0.0)\n",
    "\n",
    "        # 主对角线核\n",
    "        for i in range(kernel_size):\n",
    "            self.conv_d1.weight.data[0, 0, i, i] = 1.0\n",
    "        # 副对角线核\n",
    "        for i in range(kernel_size):\n",
    "            self.conv_d2.weight.data[0, 0, i, kernel_size - 1 - i] = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_h = self.conv_h(x)\n",
    "        out_v = self.conv_v(x)\n",
    "        out_d1 = self.conv_d1(x)\n",
    "        out_d2 = self.conv_d2(x)\n",
    "        return out_h, out_v, out_d1, out_d2\n",
    "\n",
    "# --- 生成 7x7 单通道输入图像 --- #\n",
    "def generate_input():\n",
    "    data = np.arange(49).reshape(1, 1, 7, 7).astype(np.float32)\n",
    "    return torch.from_numpy(data)\n",
    "\n",
    "# --- 打印结果为二维图像 --- #\n",
    "def print_tensor(name, tensor):\n",
    "    array = tensor.squeeze().detach().numpy()\n",
    "    print(f\"{name}:\\n{np.round(array, 1)}\\n\")\n",
    "\n",
    "# --- 主函数 --- #\n",
    "if __name__ == \"__main__\":\n",
    "    input_tensor = generate_input()\n",
    "    model = StripConvTest(kernel_size=3)\n",
    "    out_h, out_v, out_d1, out_d2 = model(input_tensor)\n",
    "\n",
    "    print_tensor(\"Input\", input_tensor)\n",
    "    print_tensor(\"Horizontal Conv\", out_h)\n",
    "    print_tensor(\"Vertical Conv\", out_v)\n",
    "    print_tensor(\"Main Diagonal Conv\", out_d1)\n",
    "    print_tensor(\"Anti Diagonal Conv\", out_d2)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class StripConvTest(nn.Module):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super().__init__()\n",
    "        pad = kernel_size // 2\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv_h = nn.Conv2d(1, 1, (1, kernel_size), padding=(0, pad), bias=False)\n",
    "        self.conv_v = nn.Conv2d(1, 1, (kernel_size, 1), padding=(pad, 0), bias=False)\n",
    "        self.conv_d1 = nn.Conv2d(1, 1, kernel_size, padding=pad, bias=False)\n",
    "        self.conv_d2 = nn.Conv2d(1, 1, kernel_size, padding=pad, bias=False)\n",
    "\n",
    "        # 初始化水平方向 & 垂直方向为全1\n",
    "        nn.init.constant_(self.conv_h.weight, 1.0)\n",
    "        nn.init.constant_(self.conv_v.weight, 1.0)\n",
    "\n",
    "        # 初始化对角线方向\n",
    "        self.d1_mask = torch.zeros_like(self.conv_d1.weight.data)\n",
    "        self.d2_mask = torch.zeros_like(self.conv_d2.weight.data)\n",
    "        for i in range(kernel_size):\n",
    "            self.d1_mask[0, 0, i, i] = 1.0\n",
    "            self.d2_mask[0, 0, i, kernel_size - 1 - i] = 1.0\n",
    "\n",
    "        self.conv_d1.weight.data *= self.d1_mask\n",
    "        self.conv_d2.weight.data *= self.d2_mask\n",
    "\n",
    "        # hook 限制梯度更新只在对角线\n",
    "        self.conv_d1.weight.register_hook(lambda grad: grad * self.d1_mask.to(grad.device))\n",
    "        self.conv_d2.weight.register_hook(lambda grad: grad * self.d2_mask.to(grad.device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv_h(x)\n",
    "        v = self.conv_v(x)\n",
    "        d1 = self.conv_d1(x)\n",
    "        d2 = self.conv_d2(x)\n",
    "        out = (h + v + d1 + d2) / 4\n",
    "        return out\n"
   ],
   "id": "9537468062681ddb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T08:24:08.822315Z",
     "start_time": "2025-07-31T08:24:02.150951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class DrConv(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel=1,kernel_size=3,padding='same',stride=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "        self.conv_h = nn.Conv2d(in_channel, out_channel, (1, kernel_size), padding=padding, bias=False,stride=stride)\n",
    "        self.conv_v = nn.Conv2d(in_channel, out_channel, (kernel_size, 1), padding=padding, bias=False,stride=stride)\n",
    "        self.conv_d1 = torch.nn.Parameter(\n",
    "            torch.randn(out_channel,in_channel,1, kernel_size), requires_grad=True\n",
    "        )\n",
    "        self.conv_d2 = torch.nn.Parameter(\n",
    "            torch.randn(out_channel,in_channel,1, kernel_size), requires_grad=True\n",
    "        )\n",
    "        nn.init.kaiming_uniform_(self.conv_d1)\n",
    "        nn.init.kaiming_uniform_(self.conv_d2)\n",
    "        self.eyes = torch.eye(kernel_size,requires_grad=False)\n",
    "        self.reyes = torch.flip(self.eyes,[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv_h(x)\n",
    "        v = self.conv_v(x)\n",
    "        d1 = F.conv2d(x,self.conv_d1*self.eyes,stride=self.stride,padding=self.padding)\n",
    "        d2 = F.conv2d(x,self.conv_d2*self.reyes,stride=self.stride,padding=self.padding)\n",
    "        return h, v, d1, d2\n"
   ],
   "id": "a08aca21d2b4bdb9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T08:25:06.137878Z",
     "start_time": "2025-07-31T08:25:06.115197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from swin import SwinBlock\n",
    "from abtb import LayerNorm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MDFusion(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.c3 = DrConv(in_channels, out_channels, kernel_size=3)\n",
    "        self.c5 = DrConv(in_channels, out_channels, kernel_size=5)\n",
    "        self.c7 = DrConv(in_channels, out_channels, kernel_size=7)\n",
    "        self.ln = LayerNorm(in_channels*4)\n",
    "        self.attn = nn.Sequential(\n",
    "            SwinBlock(dim=4 * in_channels, input_resolution=None,num_heads=4, shift_size=0),\n",
    "            SwinBlock(dim=4 * in_channels, input_resolution=None,num_heads=4, shift_size=7),\n",
    "            nn.Conv2d(4 * in_channels, in_channels, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        o11,o21,o31,o41 = self.c3(x)\n",
    "        o12,o22,o32,o42 = self.c5(x)\n",
    "        o13,o23,o33,o43 = self.c7(x)\n",
    "        o1 = torch.concat([o11, o12, o13], dim=1)\n",
    "        o2 = torch.concat([o21, o22, o23], dim=1)\n",
    "        o3 = torch.concat([o31, o32, o33], dim=1)\n",
    "        o4 = torch.concat([o41, o42, o43], dim=1)\n",
    "\n",
    "        \n",
    "        fs1= o1.view(b, 3, c, h, w).max(dim=1)[0] + o1.view(b, 3, c, h, w).mean(dim=1)\n",
    "        fs2= o2.view(b, 3, c, h, w).max(dim=1)[0] + o2.view(b, 3, c, h, w).mean(dim=1)\n",
    "        fs3= o3.view(b, 3, c, h, w).max(dim=1)[0] + o3.view(b, 3, c, h, w).mean(dim=1)\n",
    "        fs4= o4.view(b, 3, c, h, w).max(dim=1)[0] + o4.view(b, 3, c, h, w).mean(dim=1)        \n",
    "        \n",
    "        fs = torch.concat([fs1/2, fs2/2, fs3/2, fs4/2], dim=1)\n",
    "        out = self.ln(fs)\n",
    "        out = self.attn(out)\n",
    "        return x+out\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ],
   "id": "3470abf51f8d7f9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T08:25:18.565199Z",
     "start_time": "2025-07-31T08:25:17.220938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 3, 256, 256)\n",
    "model = MDFusion(in_channels=3, out_channels=3)\n",
    "x = model(x)\n",
    "x"
   ],
   "id": "bbc45d4705d13b09",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.2225e+00,  3.5067e+00,  2.0813e+00,  ...,  4.7827e-01,\n",
       "            2.3506e+00,  1.0985e+00],\n",
       "          [ 2.8899e+00, -1.4897e-02,  2.8195e-02,  ...,  1.9042e+00,\n",
       "            2.9225e-01,  1.1990e+00],\n",
       "          [ 3.0239e+00,  2.1733e+00,  8.0982e-01,  ...,  2.5089e+00,\n",
       "            2.9334e+00,  1.2483e+00],\n",
       "          ...,\n",
       "          [ 3.3210e+00,  2.2619e+00,  2.9278e+00,  ...,  1.7615e+00,\n",
       "            1.6711e+00, -8.5959e-01],\n",
       "          [ 6.6111e-01,  2.3380e+00,  2.7204e+00,  ...,  1.1581e+00,\n",
       "            3.9818e-01,  7.6475e-01],\n",
       "          [ 7.8300e-01,  6.4524e-01,  2.7036e+00,  ...,  1.2490e+00,\n",
       "            1.1947e+00,  1.0479e+00]],\n",
       "\n",
       "         [[ 8.3272e-01, -1.2717e+00, -2.0454e+00,  ..., -1.4773e+00,\n",
       "           -3.7280e-01, -2.2769e+00],\n",
       "          [-1.7073e+00, -2.6315e+00, -9.4914e-01,  ..., -1.0419e+00,\n",
       "            9.2039e-01,  5.7414e-01],\n",
       "          [-1.0306e+00,  8.5564e-01, -1.2150e+00,  ..., -5.8251e-01,\n",
       "           -1.2181e+00, -1.2888e+00],\n",
       "          ...,\n",
       "          [-1.1858e+00, -1.7045e+00, -2.8549e-01,  ..., -3.0541e-01,\n",
       "           -1.3143e+00, -1.8754e+00],\n",
       "          [ 5.4851e-02, -1.5028e+00, -1.3648e-01,  ...,  5.4852e-01,\n",
       "           -5.3685e-01, -1.8825e+00],\n",
       "          [ 3.8879e-01,  8.6310e-01, -1.5572e+00,  ...,  3.8154e-01,\n",
       "           -3.9369e-01,  3.9931e-01]],\n",
       "\n",
       "         [[-1.0270e+00, -4.9883e-01,  3.3039e-01,  ..., -7.9610e-02,\n",
       "           -1.6472e+00,  1.6143e+00],\n",
       "          [-3.2615e-01, -1.0692e+00,  1.7762e+00,  ...,  5.3068e-01,\n",
       "            3.4959e-01, -9.7845e-01],\n",
       "          [-1.3642e+00, -1.3026e+00, -3.7267e-02,  ..., -1.5665e+00,\n",
       "           -1.2109e+00, -2.6544e-01],\n",
       "          ...,\n",
       "          [-1.6787e+00, -1.1367e+00, -5.1598e-01,  ..., -1.0293e+00,\n",
       "           -2.6443e-01, -1.3601e+00],\n",
       "          [ 7.1563e-01, -6.5331e-01, -1.8886e-01,  ...,  1.3126e+00,\n",
       "            1.3106e-01, -2.9979e-01],\n",
       "          [ 4.9833e-01,  5.7192e-01, -8.6987e-01,  ...,  3.6017e-02,\n",
       "            5.3070e-01, -2.8962e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 9.1449e-01,  1.8266e+00,  1.7869e+00,  ...,  1.2961e+00,\n",
       "            9.8392e-01,  1.4888e+00],\n",
       "          [ 3.8544e-01,  6.6396e-01,  1.0725e-01,  ...,  2.4875e+00,\n",
       "            2.2530e+00,  1.7562e+00],\n",
       "          [ 2.3602e+00,  1.6344e+00, -2.9119e-02,  ...,  1.2364e+00,\n",
       "           -4.0117e-01,  1.0909e+00],\n",
       "          ...,\n",
       "          [ 9.0403e-01, -6.0442e-01,  1.3771e+00,  ...,  1.8018e+00,\n",
       "            1.1940e+00,  1.8286e+00],\n",
       "          [ 1.9959e+00, -8.3480e-01, -4.9117e-01,  ...,  2.9942e+00,\n",
       "            2.6987e+00,  1.0817e+00],\n",
       "          [ 7.4726e-01,  3.1464e+00,  4.3708e-04,  ...,  2.0319e-01,\n",
       "            1.7167e+00,  1.7525e+00]],\n",
       "\n",
       "         [[-1.0895e+00, -2.2155e+00, -4.3365e-01,  ...,  4.4041e-01,\n",
       "           -1.6785e+00, -1.5483e+00],\n",
       "          [ 8.3113e-01, -5.0553e-01, -2.6895e+00,  ...,  4.9962e-01,\n",
       "           -1.0611e+00, -1.6086e+00],\n",
       "          [ 2.5026e-01,  5.1470e-01, -9.6337e-01,  ..., -2.9121e+00,\n",
       "           -1.7276e+00,  7.1654e-01],\n",
       "          ...,\n",
       "          [ 5.3239e-01, -8.1114e-01, -1.3667e+00,  ..., -1.1416e-01,\n",
       "           -1.8821e+00, -1.0774e+00],\n",
       "          [-8.6548e-01, -2.3194e+00, -2.2986e+00,  ..., -7.7250e-01,\n",
       "           -2.1546e+00, -4.2353e-01],\n",
       "          [ 7.9077e-01, -5.0218e-01, -3.1206e-01,  ...,  9.9722e-01,\n",
       "           -2.1065e+00, -1.0191e+00]],\n",
       "\n",
       "         [[ 1.6535e-01,  9.2372e-01, -1.3984e+00,  ..., -9.2052e-02,\n",
       "           -8.6967e-01, -1.1410e+00],\n",
       "          [-8.3366e-01, -1.2673e+00,  6.8202e-01,  ..., -8.7061e-01,\n",
       "           -1.5430e+00, -7.4596e-01],\n",
       "          [ 2.1963e-03,  5.9974e-01,  2.9743e-01,  ..., -2.3224e+00,\n",
       "            1.0582e-01,  8.1661e-01],\n",
       "          ...,\n",
       "          [-1.2168e-01,  7.1583e-01, -9.7206e-02,  ..., -1.7337e+00,\n",
       "           -1.8991e+00,  2.3784e-01],\n",
       "          [-7.7959e-01, -5.1694e-01,  5.8113e-01,  ..., -2.7643e-01,\n",
       "           -4.5495e-01, -1.6378e-01],\n",
       "          [ 6.5931e-02, -7.2982e-01, -5.3162e-01,  ..., -8.1146e-01,\n",
       "            3.6828e-01, -6.5665e-01]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fca3a1b827cc5c66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_data(batch_size=16):\n",
    "    # 生成 16 张 7x7 图像，数值为0~1\n",
    "    x = torch.rand(batch_size, 3, 7, 7)\n",
    "    # 目标为全1图像\n",
    "    y = torch.ones_like(x)\n",
    "    return x, y\n"
   ],
   "id": "e74699faca3e183b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c91432df4dfeeabf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train():\n",
    "    model = StripConvTest(in_channel=3, out_channel=3, kernel_size=3)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    print(\"Initial d1 kernel:\")\n",
    "    print(model.conv_d1)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model.train()\n",
    "        x, y = generate_data()\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # 强制保持 mask（保险措施）\n",
    "        # with torch.no_grad():\n",
    "        #     model.conv_d1.weight.data *= model.d1_mask\n",
    "        #     model.conv_d2.weight.data *= model.d2_mask\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:02d}: Loss={loss.item():.6f}\")\n",
    "\n",
    "    print(\"\\nAfter training d1 kernel:\")\n",
    "    print(model.conv_d1)\n"
   ],
   "id": "3d60d1eca14b0d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "id": "cba6707ad3324baf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x=torch.FloatTensor([[[[1,1,1]],\n",
    "                      [[1,2,2,]]],\n",
    "                     [[[2,1,1]],\n",
    "                     [[2,2,2]]]])\n",
    "print(x)\n",
    "y=torch.eye(3)\n",
    "print(y)\n",
    "z=torch.matmul(x, y)\n",
    "print(z)\n",
    "z =x*y\n",
    "print(z)"
   ],
   "id": "14fae596a84258ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x=torch.nn.Parameter(torch.randn(2,1,3))\n",
    "x"
   ],
   "id": "76dfc5798fb3573b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y",
   "id": "b88d18ad16087846",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(torch.flip(y, dims=[-1]))\n",
    "y"
   ],
   "id": "51af9bcd1a7f27a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T12:06:52.559987Z",
     "start_time": "2025-08-06T12:06:52.384612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "p = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "x = torch.randn(1, 1, 5, 5)\n",
    "out = -p(-x)\n",
    "print(\"Input:\\n\", x)\n",
    "print(\"Output:\\n\", out)"
   ],
   "id": "6f43838b3bf7c2f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[[[-0.8645, -1.3713, -0.4421,  0.3379,  0.4282],\n",
      "          [-1.1967, -1.4151,  0.0986,  0.7842, -0.6357],\n",
      "          [-1.1211, -1.0270,  0.4290, -0.9869,  1.4205],\n",
      "          [ 0.6235, -0.4959,  0.7135, -1.4688,  0.0828],\n",
      "          [ 0.9833,  0.7404,  0.1762,  0.8674, -0.0630]]]])\n",
      "Output:\n",
      " tensor([[[[-1.4151, -1.4151, -1.4151, -0.6357, -0.6357],\n",
      "          [-1.4151, -1.4151, -1.4151, -0.9869, -0.9869],\n",
      "          [-1.4151, -1.4151, -1.4688, -1.4688, -1.4688],\n",
      "          [-1.1211, -1.1211, -1.4688, -1.4688, -1.4688],\n",
      "          [-0.4959, -0.4959, -1.4688, -1.4688, -1.4688]]]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1ef6a3e4b44e6b24"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
